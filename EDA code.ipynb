{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno\n",
    "%matplotlib inline\n",
    "\n",
    "# Set pandas output display to have one digit for decimal places and limit it to\n",
    "# printing 15 rows.\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.options.display.max_rows = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading multiple data source\n",
    "#Read files\n",
    "filenames = ['partnership_map.csv', 'touchpoints.csv','trades.csv']\n",
    "dataframes = [pd.read_csv(f) for f in filenames] \n",
    "#call files on the list\n",
    "pm=dataframes[0]\n",
    "tp=dataframes[1]\n",
    "tr=dataframes[2] \n",
    "\n",
    "#training and validation\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "#Or just do cross validation\n",
    "\n",
    "#EDA\n",
    "\n",
    "\n",
    "#rank missing values desc\n",
    "def missing_df(df):\n",
    "    missing_df=pd.DataFrame(df.apply(lambda x:sum(x.isnull())/len(df)))\n",
    "    missing_df.columns=['pct_missing']\n",
    "    missing_df=missing_df.sort_values(by='pct_missing', ascending=False)\n",
    "    return missing_df\n",
    "#dtypes and descriptions after removing missing values\n",
    "def data_types(df):\n",
    "  info=df.describe()\n",
    "  return display(df.dtypes, info)\n",
    "\n",
    "#drop duplicates\n",
    "def drop_dup(df):\n",
    "  df=df.drop_duplicates()\n",
    "  return df\n",
    "\n",
    "#reindex\n",
    "def reindex(df):    \n",
    "    df=df.reindex(np.random.permutation(df.index))\n",
    "    return df\n",
    "\n",
    "#Deleting variables that are missing more than 90% and drop duplicates\n",
    "  def drop_missing_dup(df):\n",
    "    drop_missing=df.dropna(thresh=len(df)*0.9, axis='columns')\n",
    "    drop_missing_dup=drop_missing.drop_duplicates()\n",
    "    return drop_missing_dup\n",
    "\n",
    "#seperate number and strings...\n",
    "def number(self):\n",
    "    df_number=df.select_dtypes(include='number')\n",
    "    df_object=df.select_dtypes(include='object')\n",
    "    return df_number, df_object\n",
    "\n",
    "\n",
    "#descending sorting by col\n",
    "def sort(df, col):\n",
    "    return df.sort_values(by=col, ascending=False)\n",
    "   \n",
    "   # get dummy variables\n",
    "\n",
    "def get_dummies(df_object, df_number):\n",
    "    for i in range(len(df_object.columns)):\n",
    "        z=pd.get_dummies(df_object.iloc[:, i], prefix=df_object.columns[i]) #create variables with new names\n",
    "        df_object=df_object.join(z)\n",
    "        df_model=df_number.join(df_object) #join both numeric and OHEed cols.\n",
    "    return df_model.select_dtypes(include='number')\n",
    "\n",
    "def num_unique(df): #count number of levels for each variable\n",
    "    return pd.DataFrame(df.apply(lambda column:column.nunique()) )\n",
    "    \n",
    "def low_variance(df):\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    def variance_threshold(df, threshold=0.8):\n",
    "        selector=VarianceThreshold(threshold)\n",
    "        selector.fit(df)\n",
    "        return df[df.columns[selector.get_support(indices= True)]]\n",
    "    return [i for i in df.columns if i not in variance_threshold(df).columns] #return variables that need to be dropped. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histograms_numeric_columns(df, numerical_columns):\n",
    "    '''\n",
    "    Takes df, numerical columns as list\n",
    "    Returns a group of histagrams\n",
    "    '''\n",
    "    f = pd.melt(df, value_vars=numerical_columns) \n",
    "    g = sns.FacetGrid(f, col='variable',  col_wrap=4, sharex=False, sharey=False)\n",
    "    g = g.map(sns.distplot, 'value')\n",
    "    return g\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General Data considerations:\n",
    "1. Binary classifications:\n",
    "    a. target variable: is it a rare event?\n",
    "    b. how much money is on the line? which can you tolerate more: false positives or false negatives...\n",
    "    c. Feature variables: EDA, missing values, outliers (log transformation), visuals, scales, multicollinearity, coded correctly? binning..low        variance ("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Baseline logistic Regression model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "log_reg = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "log_reg.fit(X_train, y_train) #use target1\n",
    "logreg_predictions1_s = log_reg.predict(X_valid)\n",
    "precision_score(y_valid, logreg_predictions1_s)\n",
    "\n",
    "\n",
    "#Baseline Linear Regression model\n",
    "#retrieve coefficients\n",
    "regressor = LinearRegression()  \n",
    "regressor.fit(X_train, y_train) #training the algorithm\n",
    "regressor.score(X, y)\n",
    "coeff_df = pd.DataFrame(regressor.coef_, X_train.columns, columns=['Coefficient'])  \n",
    "coeff_df\n",
    "\n",
    "#prediction and measure performance \n",
    "lineareg_predictions1_s = regressor.predict(X_valid)\n",
    "mean_squared_error(y_valid, lineareg_predictions1_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Xgboost Classification\n",
    "import xgboost as xgb\n",
    "model1 = xgb.XGBClassifier(silent=False, \n",
    "                      scale_pos_weight=1,\n",
    "                      learning_rate=0.1,  \n",
    "                      colsample_bytree = 0.6,\n",
    "                      subsample = 0.8,\n",
    "                      objective='binary:logistic', \n",
    "                      n_estimators=200, \n",
    "                      reg_alpha = 0.3,\n",
    "                      max_depth=6, \n",
    "                      gamma=10,\n",
    "                      seed=42\n",
    "                    )\n",
    "eval_set_ns = [(X_train, y_train), (X_valid, y_valid)]\n",
    "\n",
    "\n",
    "%time model1.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=[\"logloss\", 'error'], eval_set=eval_set_ns, verbose=True)\n",
    "\n",
    "#plot the training \n",
    "epochs = len(results['validation_0']['error'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['logloss'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['logloss'], label='Valid')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Log Loss')\n",
    "pyplot.title('XGBoost Log Loss')\n",
    "pyplot.show()\n",
    "# plot classification error\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['error'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['error'], label='Valid')\n",
    "ax.legend()\n",
    "pyplot.ylabel('Classification Error')\n",
    "pyplot.title('XGBoost Classification Error')\n",
    "pyplot.show()\n",
    "\n",
    "xgb_predict= model1.predict(valid)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "precision_score(y_valid, xgb_predict)\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(y_valid, xgb_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "# regression \n",
    "model1 = xgb.XGBRegressor(silent=False, \n",
    "                      scale_pos_weight=1,\n",
    "                      learning_rate=0.01,  \n",
    "                      colsample_bytree = 0.6,\n",
    "                      subsample = 0.8,\n",
    "                       \n",
    "                      n_estimators=500, \n",
    "                      reg_alpha = 0.3,\n",
    "                      max_depth=6, \n",
    "                      gamma=10,\n",
    "                      seed=42\n",
    "\n",
    "                    )\n",
    "eval_set = [(X_train, y_train), (X_valid, y_valid)]\n",
    "\n",
    "#change evaluation criteria to 'RMSE'\n",
    "model1.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=[\"rmse\"], eval_set=eval_set, verbose=False)\n",
    "results =model1.evals_result()\n",
    "\n",
    "\n",
    "from matplotlib import pyplot\n",
    "epochs = len(results['validation_0']['rmse'])\n",
    "x_axis = range(0, epochs)\n",
    "# plot log loss\n",
    "fig, ax = pyplot.subplots()\n",
    "ax.plot(x_axis, results['validation_0']['rmse'], label='Train')\n",
    "ax.plot(x_axis, results['validation_1']['rmse'], label='Valid')\n",
    "ax.legend()\n",
    "pyplot.ylabel('RMSE')\n",
    "pyplot.title('XGBoost RMSE')\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "#%time model1.fit(X_train, y_train, early_stopping_rounds=5, eval_set=eval_set_ns, verbose=True)\n",
    "xgb_predict= model1.predict(X_valid)\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# #mean_squared_error(y_valid, xgb_predict)\n",
    "\n",
    "from math import sqrt\n",
    "\n",
    "rmse = sqrt(mean_squared_error(y_valid, xgb_predict))\n",
    "print (rmse)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_scores(scores):\n",
    "    print(\"Scores: {0}\\nMean: {1:.3f}\\nStd: {2:.3f}\".format(scores, np.mean(scores), np.std(scores)))\n",
    "\n",
    "# diabetes = load_diabetes()\n",
    "# X = diabetes.data\n",
    "# y = diabetes.target\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "scores = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):   \n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "                      objective=\"reg:squarederror\",   #or 'binary:logistic',                \n",
    "                      seed=42)\n",
    "    eval_set = [(X_train, y_train), (X_test, y_test)]\n",
    "    xgb_model.fit(X_train, y_train, early_stopping_rounds=5, eval_metric=[\"rmse\"], #or logloss, \n",
    "                  eval_set=eval_set, verbose=False)\n",
    "    \n",
    "    y_pred = xgb_model.predict(X_test)\n",
    "    \n",
    "    scores.append(mean_squared_error(y_test, y_pred)) #or whatever\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "display_scores(np.sqrt(scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def construct_feature_columns(input_features):\n",
    "      return set([tf.feature_column.numeric_column(my_feature)\n",
    "              for my_feature in input_features])\n",
    "def my_input_fn(features, targets, batch_size=1, shuffle=True, num_epochs=None):\n",
    "\n",
    "    # Convert pandas data into a dict of np arrays.\n",
    "    features = {key:np.array(value) for key,value in dict(features).items()}                                             \n",
    " \n",
    "    # Construct a dataset, and configure batching/repeating.\n",
    "    ds = Dataset.from_tensor_slices((features,targets)) # warning: 2GB limit\n",
    "    ds = ds.batch(batch_size).repeat(num_epochs)\n",
    "    \n",
    "    # Shuffle the data, if specified.\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(10000)\n",
    "    \n",
    "    # Return the next batch of data.\n",
    "    features, labels = ds.make_one_shot_iterator().get_next()\n",
    "    return features, labels\n",
    "\n",
    "\n",
    "def train_nn_regression_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "      periods = 10\n",
    "    steps_per_period = steps / periods\n",
    "  \n",
    "  # Create a DNNRegressor object.\n",
    "  my_optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "  my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "  dnn_regressor = tf.estimator.DNNRegressor(\n",
    "      feature_columns=construct_feature_columns(training_examples),\n",
    "      hidden_units=hidden_units,\n",
    "      optimizer=my_optimizer,\n",
    "  )\n",
    "  \n",
    "  # Create input functions.\n",
    "  training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                          training_targets, \n",
    "                                          batch_size=batch_size)\n",
    "  predict_training_input_fn = lambda: my_input_fn(training_examples, \n",
    "                                                  training_targets, \n",
    "                                                  num_epochs=1, \n",
    "                                                  shuffle=False)\n",
    "  predict_validation_input_fn = lambda: my_input_fn(validation_examples, \n",
    "                                                    validation_targets, \n",
    "                                                    num_epochs=1, \n",
    "                                                    shuffle=False)\n",
    "\n",
    "  # Train the model, but do so inside a loop so that we can periodically assess\n",
    "  # loss metrics.\n",
    "  print(\"Training model...\")\n",
    "  print(\"RMSE (on training data):\")\n",
    "  training_rmse = []\n",
    "  validation_rmse = []\n",
    "  for period in range (0, periods):\n",
    "    # Train the model, starting from the prior state.\n",
    "    dnn_regressor.train(\n",
    "        input_fn=training_input_fn,\n",
    "        steps=steps_per_period\n",
    "    )\n",
    "    # Take a break and compute predictions.\n",
    "    training_predictions = dnn_regressor.predict(input_fn=predict_training_input_fn)\n",
    "    training_predictions = np.array([item['predictions'][0] for item in training_predictions])\n",
    "    \n",
    "    validation_predictions = dnn_regressor.predict(input_fn=predict_validation_input_fn)\n",
    "    validation_predictions = np.array([item['predictions'][0] for item in validation_predictions])\n",
    "    \n",
    "    # Compute training and validation loss.\n",
    "    training_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(training_predictions, training_targets))\n",
    "    validation_root_mean_squared_error = math.sqrt(\n",
    "        metrics.mean_squared_error(validation_predictions, validation_targets))\n",
    "    # Occasionally print the current loss.\n",
    "    print(\"  period %02d : %0.2f\" % (period, training_root_mean_squared_error))\n",
    "    # Add the loss metrics from this period to our list.\n",
    "    training_rmse.append(training_root_mean_squared_error)\n",
    "    validation_rmse.append(validation_root_mean_squared_error)\n",
    "  print(\"Model training finished.\")\n",
    "\n",
    "  # Output a graph of loss metrics over periods.\n",
    "  plt.ylabel(\"RMSE\")\n",
    "  plt.xlabel(\"Periods\")\n",
    "  plt.title(\"Root Mean Squared Error vs. Periods\")\n",
    "  plt.tight_layout()\n",
    "  plt.plot(training_rmse, label=\"training\")\n",
    "  plt.plot(validation_rmse, label=\"validation\")\n",
    "  plt.legend()\n",
    "\n",
    "  print(\"Final RMSE (on training data):   %0.2f\" % training_root_mean_squared_error)\n",
    "  print(\"Final RMSE (on validation data): %0.2f\" % validation_root_mean_squared_error)\n",
    "\n",
    "  return dnn_regressor\n",
    "\n",
    "\n",
    "dnn_regressor = train_nn_regression_model(\n",
    "    learning_rate=0.01,\n",
    "    steps=200,\n",
    "    batch_size=100,\n",
    "    hidden_units=[10, 10],\n",
    "    training_examples=X_train,\n",
    "    training_targets=y_train,\n",
    "    validation_examples=X_valid,\n",
    "    validation_targets=y_valid)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
